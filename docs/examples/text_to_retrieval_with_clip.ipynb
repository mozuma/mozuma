{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-image retrieval with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://github.com/mozuma/mozuma/blob/master/docs/examples/text_to_retrieval_with_clip.ipynb\">\n",
    "  <img src=\"https://img.shields.io/static/v1?label=&message=See%20the%20source%20code&color=blue&logo=github&labelColor=black\" alt=\"See the source code\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mozuma/mozuma/blob/master/docs/examples/text_to_retrieval_with_clip.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This is an example of a text-to-Image retrieval engine based on OpenAI CLIP model.\n",
    "\n",
    "Import `mozuma` modules for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozuma.torch.runners import TorchInferenceRunner\n",
    "from mozuma.torch.options import TorchRunnerOptions\n",
    "from mozuma.callbacks.memory import (\n",
    "    CollectFeaturesInMemory,\n",
    ")\n",
    "from mozuma.torch.datasets import (\n",
    "    ImageDataset,\n",
    "    ListDataset,\n",
    "    LocalBinaryFilesDataset,\n",
    ")\n",
    "from mozuma.helpers.files import list_files_in_dir\n",
    "\n",
    "from mozuma.models.clip.text import CLIPTextModule\n",
    "from mozuma.models.clip.image import CLIPImageModule\n",
    "\n",
    "from mozuma.states import StateKey\n",
    "from mozuma.stores import Store\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CLIP Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CLIPImageModule(clip_model_name=\"ViT-B/32\", device=torch.device(\"cuda\"))\n",
    "store = Store()\n",
    "store.load(image_encoder, StateKey(image_encoder.state_type, \"clip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract CLIP image features of FlickR30k dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might take a few minutes for extracting the features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_flickr30k_images = \"/mnt/storage01/datasets/flickr30k/full/images\"\n",
    "file_names = list_files_in_dir(path_to_flickr30k_images, allowed_extensions=(\"jpg\",))\n",
    "dataset = ImageDataset(LocalBinaryFilesDataset(file_names))\n",
    "\n",
    "image_features = CollectFeaturesInMemory()\n",
    "runner = TorchInferenceRunner(\n",
    "    dataset=dataset,\n",
    "    model=image_encoder,\n",
    "    callbacks=[image_features],\n",
    "    options=TorchRunnerOptions(\n",
    "        data_loader_options={\"batch_size\": 128},\n",
    "        device=image_encoder.device,\n",
    "        tqdm_enabled=True,\n",
    "    ),\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CLIP Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModule(image_encoder.clip_model_name, device=torch.device(\"cpu\"))\n",
    "store.load(text_encoder, StateKey(text_encoder.state_type, \"clip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract CLIP text features of a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_queries = [\n",
    "    \"Workers look down from up above on a piece of equipment .\",\n",
    "    \"Ballet dancers in a studio practice jumping with wonderful form .\",\n",
    "]\n",
    "dataset = ListDataset(text_queries)\n",
    "\n",
    "text_features = CollectFeaturesInMemory()\n",
    "runner = TorchInferenceRunner(\n",
    "    dataset=dataset,\n",
    "    model=text_encoder,\n",
    "    callbacks=[text_features],\n",
    "    options=TorchRunnerOptions(\n",
    "        data_loader_options={\"batch_size\": 1},\n",
    "        device=text_encoder.device,\n",
    "        tqdm_enabled=True,\n",
    "    ),\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-image retrieval engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the top 5 most similar images for the text query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "img_feat = torch.tensor(image_features.features).type(torch.float32)\n",
    "img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "txt_feat = torch.tensor(text_features.features)\n",
    "txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * txt_feat @ img_feat.T).softmax(dim=-1)\n",
    "values, indices = similarity.topk(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install ipyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyplot\n",
    "from PIL import Image\n",
    "\n",
    "for k, text in enumerate(text_queries):\n",
    "    print(f\"Query: {text}\")\n",
    "    print(f\"Top 5 images:\")\n",
    "    ipyplot.plot_images(\n",
    "        [Image.open(image_features.indices[i]) for i in indices[k]],\n",
    "        [f\"{v*100:.1f}%\" for v in values[k]],\n",
    "        img_width=250,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "446fa4ae14fc0db07deeb22d284bdd26ac4d0fa6060970ce5f4cfd209622d30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
